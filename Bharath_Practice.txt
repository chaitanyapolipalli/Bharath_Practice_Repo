1.

export SPARK_MAJOR_VERSION=2
spark-shell

val hr_data = spark.read.format("csv").option("header", "true").option("inferSchema", "true").load("/user/chaitanyapolipalli/bharath/HR_Analysis_Dataset.csv")

hr_data.createOrReplaceTempView("hrData")

spark.sql("select sales, round(avg(satisfaction_level),2) as Avg from hrData group by sales order by Avg").show
spark.sql("select sales, round(avg(satisfaction_level),2) as Avg from hrData group by sales order by Avg desc").show

+-----------+------------------+
|      sales|               Avg|
+-----------+------------------+
| accounting|0.5821512385919166|
|         hr|0.5988092016238159|
|  technical|0.6078970588235295|
|      sales| 0.614446859903383|
|         IT|0.6181418092909551|
|    support|0.6182996859578297|
|  marketing|0.6186013986013983|
|product_mng|0.6196341463414636|
|      RandD|0.6198221092757306|
| management|0.6213492063492058|
+-----------+------------------+

#####################################################################################################################

2.

spark.sql("select sales, count(left) as Total_Left from hrData where left = 0 group by sales order by Total_Left ").show
spark.sql("select sales, count(left) as Total_Left from hrData where left = 0 group by sales order by Total_Left desc").show
+-----------+----------+
|      sales|Total_Left|
+-----------+----------+
|         hr|       524|
| management|       539|
| accounting|       563|
|  marketing|       655|
|      RandD|       666|
|product_mng|       704|
|         IT|       954|
|    support|      1674|
|  technical|      2023|
|      sales|      3126|
+-----------+----------+
#####################################################################################################################

3.

spark.sql("select sales, round(avg(average_montly_hours),2) as Avg_Hours from hrData group by sales order by Avg_Hours").show
spark.sql("select sales, round(avg(average_montly_hours),2) as Avg_Hours from hrData group by sales order by Avg_Hours desc").show

+-----------+------------------+
|      sales|         Avg_Hours|
+-----------+------------------+
|         hr|198.68470906630583|
|  marketing| 199.3857808857809|
|product_mng|199.96563192904657|
|    support|200.75818752803949|
|      RandD| 200.8005082592122|
|      sales|200.91135265700484|
| accounting|201.16297262059973|
| management|201.24920634920636|
|         IT| 202.2159739201304|
|  technical|202.49742647058824|

#####################################################################################################################

4.

spark.sql("select sales, count(number_project) as Total_Projects from hrData group by sales order by Total_Projects ").show
spark.sql("select sales, count(number_project) as Total_Projects from hrData group by sales order by Total_Projects desc").show
+-----------+--------------+
|      sales|Total_Projects|
+-----------+--------------+
| management|           630|
|         hr|           739|
| accounting|           767|
|      RandD|           787|
|  marketing|           858|
|product_mng|           902|
|         IT|          1227|
|    support|          2229|
|  technical|          2720|
|      sales|          4140|
+-----------+--------------+
#####################################################################################################################

5.
spark.sql("select salary, sales , count(salary) as Total from hrData group by salary, sales order by sales,salary").show(1000)
spark.sql("select salary, sales , count(salary) as Total from hrData group by salary, sales order by sales desc,salary").show(1000)
+------+-----------+-----+
|salary|      sales|Total|
+------+-----------+-----+
|  high|         IT|   83|
|   low|         IT|  609|
|medium|         IT|  535|
|  high|      RandD|   51|
|   low|      RandD|  364|
|medium|      RandD|  372|
|  high| accounting|   74|
|   low| accounting|  358|
|medium| accounting|  335|
|  high|         hr|   45|
|   low|         hr|  335|
|medium|         hr|  359|
|  high| management|  225|
|   low| management|  180|
|medium| management|  225|
|  high|  marketing|   80|
|   low|  marketing|  402|
|medium|  marketing|  376|
|  high|product_mng|   68|
|   low|product_mng|  451|
|medium|product_mng|  383|
|  high|      sales|  269|
|   low|      sales| 2099|
|medium|      sales| 1772|
|  high|    support|  141|
|   low|    support| 1146|
|medium|    support|  942|
|  high|  technical|  201|
|   low|  technical| 1372|
|medium|  technical| 1147|
+------+-----------+-----+

#####################################################################################################################


6.

spark.sql("select sales, count(left) as Total_Left from hrData where promotion_last_5years = 1 and left = 0 group by sales order by Total_Left ").show
spark.sql("select sales, count(left) as Total_Left from hrData where promotion_last_5years = 1 and left = 0 group by sales order by Total_Left desc").show
+----------+----------+
|     sales|Total_Left|
+----------+----------+
|accounting|        14|
|        hr|        15|
|   support|        17|
| technical|        25|
|     RandD|        27|
| marketing|        43|
|management|        66|
|     sales|        93|
+----------+----------+

#####################################################################################################################

7.

spark.sql("select sales, round(avg(satisfaction_level),2) as Avg_Satisfaction, round(avg(average_montly_hours),2) as Avg_Working_Hours, count(left) as Total_Left from hrData where left = 0 group by sales order by Avg_Satisfaction,Avg_Working_Hours,Total_Left ").show
spark.sql("select sales, round(avg(satisfaction_level),2) as Avg_Satisfaction, round(avg(average_montly_hours),2) as Avg_Working_Hours, count(left) as Total_Left from hrData where left = 0 group by sales order by Avg_Satisfaction desc,Avg_Working_Hours,Total_Left").show
+-----------+------------------+------------------+----------+
|      sales|  Avg_Satisfaction| Avg_Working_Hours|Total_Left|
+-----------+------------------+------------------+----------+
| accounting|0.6472113676731801| 199.0373001776199|       563|
|      RandD|0.6537987987987991|198.95195195195194|       666|
| management|0.6548608534322814|200.23376623376623|       539|
|product_mng|0.6584659090909087|        197.765625|       704|
|         hr|0.6666793893129773|            199.25|       524|
|  technical|0.6683193277310949|198.47108255066732|      2023|
|      sales|0.6685476647472844|199.57165706973768|      3126|
|  marketing|0.6698778625954191| 198.8885496183206|       655|
|    support|0.6737992831541231| 199.1409796893668|      1674|
|         IT|0.6771698113207558|198.88679245283018|       954|
+-----------+------------------+------------------+----------+


#####################################################################################################################

8.
spark.sql("select sales, salary, round(sum(satisfaction_level)/count(satisfaction_level),2) as Mean_Satisfaction, round(avg(average_montly_hours),2) as Avg_Working_Hours, count(left) as Total_Left from hrData where salary = 'low' and left = 0 group by sales, salary order by Mean_Satisfaction,Avg_Working_Hours,Total_Left ").show
spark.sql("select sales, salary, round(sum(satisfaction_level)/count(satisfaction_level),2) as Mean_Satisfaction, round(avg(average_montly_hours),2) as Avg_Working_Hours, count(left) as Total_Left from hrData where salary = 'low' and left = 0 group by sales, salary order by Mean_Satisfaction desc,Avg_Working_Hours,Total_Left ").show
+-----------+------+------------------+------------------+----------+
|      sales|salary| Mean_Satisfaction| Avg_Working_Hours|Total_Left|
+-----------+------+------------------+------------------+----------+
| accounting|   low|0.6432818532818535|198.85714285714286|       259|
|      RandD|   low|0.6512944983818767|197.47249190938513|       309|
|         hr|   low|0.6585596707818927|201.79423868312756|       243|
|product_mng|   low|0.6619075144508663|197.60693641618496|       346|
|  technical|   low|0.6688329979879283|198.67203219315894|       994|
|      sales|   low|0.6693651925820254| 198.5820256776034|      1402|
|    support|   low|0.6734346103038311|196.51519154557462|       757|
|  marketing|   low|0.6758333333333335|203.45652173913044|       276|
| management|   low|0.6804958677685952| 202.1900826446281|       121|
|         IT|   low|0.6816475972540049|197.90389016018307|       437|
+-----------+------+------------------+------------------+----------+

#####################################################################################################################

9.

 spark.sql("select sales,promotion_last_5years, round(sum(satisfaction_level)/count(satisfaction_level),2) as Mean_Satisfaction, round(avg(average_montly_hours),2) as Avg_Working_Hours, count(left) as Total_Left from hrData where salary = 'low' and promotion_last_5years = 1 and left = 0 group by sales, promotion_last_5years order by Mean_Satisfaction,Avg_Working_Hours,Total_Left").show
 spark.sql("select sales,promotion_last_5years, round(sum(satisfaction_level)/count(satisfaction_level),2) as Mean_Satisfaction, round(avg(average_montly_hours),2) as Avg_Working_Hours, count(left) as Total_Left from hrData where salary = 'low' and promotion_last_5years = 1 and left = 0 group by sales, promotion_last_5years order by Mean_Satisfaction desc,Avg_Working_Hours,Total_Left").show
+----------+---------------------+------------------+------------------+----------+
|     sales|promotion_last_5years| Mean_Satisfaction| Avg_Working_Hours|Total_Left|
+----------+---------------------+------------------+------------------+----------+
|accounting|                    1|              0.52|             261.0|         2|
|     RandD|                    1|0.5499999999999999|169.33333333333334|         3|
|management|                    1|              0.58|             236.5|         4|
| technical|                    1| 0.658888888888889|184.77777777777777|         9|
|     sales|                    1|0.6799999999999999|184.28571428571428|        14|
|   support|                    1|0.6985714285714285|168.85714285714286|         7|
| marketing|                    1|0.7509090909090909| 211.1818181818182|        11|
|        hr|                    1|              0.78|             175.0|         2|
+----------+---------------------+------------------+------------------+----------+

#####################################################################################################################
10.

val count_total = spark.sql(s"""SELECT CAST(count(left) AS Int) FROM hrData""").first.get(0)

spark.sql(s"""select salary, round(avg(satisfaction_level),2) as Avg_Satisfaction_Level, round(avg(last_evaluation),2) as Avg_Last_Evaluation, count(left) as Total_Left, round(count(left)/$count_total * 100,2) as Percentage from hrData where left = 0 group by salary order by Percentage""").show

spark.sql(s"""select salary, round(avg(satisfaction_level),2) as Avg_Satisfaction_Level, round(avg(last_evaluation),2) as Avg_Last_Evaluation, count(left) as Total_Left, round(count(left)/$count_total * 100,2) as Percentage from hrData where left = 0 group by salary order by Percentage desc""").show


+------+----------------------+-------------------+----------+----------+
|salary|Avg_Satisfaction_Level|Avg_Last_Evaluation|Total_Left|Percentage|
+------+----------------------+-------------------+----------+----------+
|  high|                  0.65|               0.71|      1155|       7.7|
|   low|                  0.67|               0.72|      5144|      34.3|
|medium|                  0.67|               0.72|      5129|      34.2|
+------+----------------------+-------------------+----------+----------+


#####################################################################################################################

11.

 spark.sql("select sales, time_spend_company , count(left) as Total_Left from hrData where left = 0 group by time_spend_company,sales order by time_spend_company,Total_left").show(1000)
  spark.sql("select sales, time_spend_company , count(left) as Total_Left from hrData where left = 0 group by time_spend_company,sales order by time_spend_company desc,Total_left").show(1000)
+-----------+------------------+----------+
|      sales|time_spend_company|Total_Left|
+-----------+------------------+----------+
| management|                 2|       113|
| accounting|                 2|       139|
|         hr|                 2|       162|
|  marketing|                 2|       171|
|      RandD|                 2|       179|
|product_mng|                 2|       201|
|         IT|                 2|       264|
|  technical|                 6|       105|
|    support|                 2|       504|
|  technical|                 2|       580|
|      sales|                 2|       878|
| management|                 3|       200|
|         hr|                 3|       218|
| accounting|                 3|       243|
|  marketing|                 3|       276|
|      RandD|                 3|       296|
|product_mng|                 3|       298|
|         IT|                 3|       408|
|    support|                 3|       732|
|  technical|                 3|       896|
|      sales|                 3|      1290|
| management|                 4|        59|
|         hr|                 4|        71|
| accounting|                 4|        99|
|      RandD|                 4|       102|
|  marketing|                 4|       103|
|product_mng|                 4|       108|
|         IT|                 4|       139|
|    support|                 4|       241|
|  technical|                 4|       303|
|      sales|                 4|       442|
| management|                 5|        27|
|  marketing|                 5|        30|
| accounting|                 5|        35|
|         hr|                 5|        39|
|product_mng|                 5|        39|
|      RandD|                 5|        42|
|         IT|                 5|        60|
|    support|                 5|        98|
|  technical|                 5|       107|
|      sales|                 5|       163|
| management|                 6|        18|
|product_mng|                 6|        22|
| accounting|                 6|        23|
|         hr|                 6|        28|
|  marketing|                 6|        33|
|      RandD|                 6|        37|
|         IT|                 6|        45|
|    support|                 6|        61|
|  technical|                 6|       105|
|      sales|                 6|       137|
|  technical|                 7|         2|
|  marketing|                 7|        10|
|         IT|                 7|        12|
|product_mng|                 7|        18|
| management|                 7|        36|
|      sales|                 7|       110|
|         hr|                 8|         6|
|product_mng|                 8|         8|
|         IT|                 8|        10|
|      RandD|                 8|        10|
|  technical|                 8|        10|
|    support|                 8|        12|
| accounting|                 8|        14|
|  marketing|                 8|        20|
| management|                 8|        24|
|      sales|                 8|        48|
|product_mng|                10|        10|
| accounting|                10|        10|
|  marketing|                10|        12|
|         IT|                10|        16|
|  technical|                10|        20|
|    support|                10|        26|
|      sales|                10|        58|
| management|                10|        62|
+-----------+------------------+----------+

#####################################################################################################################

12.

val count_left = spark.sql(s"""SELECT CAST(count(left) AS Int) FROM hrData""").first.get(0)

spark.sql(s"""select sales, round((count(left)/$count_left * 100),2)as Total_Employees, count(left)/$count_left * 100 > 70 as Total_Left_More_Than_70 from hrData group by sales order by Total_Employees""").show
spark.sql(s"""select sales, round((count(left)/$count_left * 100),2)as Total_Employees, count(left)/$count_left * 100 > 70 as Total_Left_More_Than_70 from hrData group by sales order by Total_Employees desc""").show
+-----------+------------------+-----------------------+
|      sales|   Total_Employees|Total_Left_More_Than_70|
+-----------+------------------+-----------------------+
|         hr| 4.926995133008867|                  false|
| management| 4.200280018667911|                  false|
| accounting| 5.113674244949664|                  false|
|product_mng|6.0137342489499295|                  false|
|         IT| 8.180545369691313|                  false|
|    support|14.860990732715514|                  false|
|      sales|27.601840122674844|                  false|
|  marketing|  5.72038135875725|                  false|
|      RandD| 5.247016467764517|                  false|
|  technical| 18.13454230282019|                  false|
+-----------+------------------+-----------------------+

#####################################################################################################################

13.
val max_prjts = spark.sql(s"""SELECT CAST(MAX(number_project) AS Int) FROM hrData""").first.get(0)
val hours = spark.sql(s"""select CAST(MAX(average_montly_hours) AS Double) from hrData""").first.get(0)
spark.sql(s"""select distinct sales from hrData where number_project = $max_prjts and average_montly_hours = $hours group by sales""").show
+---------+
|    sales|
+---------+
|marketing|
|    sales|
|       hr|
|  support|
+---------+
#####################################################################################################################

14.
spark.sql("select time_spend_company , count(salary) as Salary_Distribution, salary from hrData group by time_spend_company,salary order by time_spend_company,salary").show(100)
spark.sql("select time_spend_company , count(salary) as Salary_Distribution, salary from hrData group by time_spend_company,salary order by time_spend_company desc,salary").show(100)
+------------------+-------------------+------+
|time_spend_company|Salary_Distribution|salary|
+------------------+-------------------+------+
|                 2|                303|  high|
|                 2|               1527|   low|
|                 2|               1414|medium|
|                 3|                520|  high|
|                 3|               3205|   low|
|                 3|               2718|medium|
|                 4|                173|  high|
|                 4|               1300|   low|
|                 4|               1084|medium|
|                 5|                 66|  high|
|                 5|                799|   low|
|                 5|                608|medium|
|                 6|                 55|  high|
|                 6|                333|   low|
|                 6|                330|medium|
|                 7|                 38|  high|
|                 7|                 36|   low|
|                 7|                114|medium|
|                 8|                 18|  high|
|                 8|                 60|   low|
|                 8|                 84|medium|
|                10|                 64|  high|
|                10|                 56|   low|
|                10|                 94|medium|
+------------------+-------------------+------+

#####################################################################################################################

15.

val count_prjts = spark.sql(s"""SELECT CAST(count(number_project) AS Int) FROM hrData""").first.get(0)

spark.sql(s"""select sales, round((count(number_project)/$count_prjts * 100),2) as Total_Project_Per_Department_Share, count(number_project)/($count_prjts) * 100 > 40 as Total_Projects_Greater_Than_40 from hrData group by sales order by Total_Project_Per_Department_Share""").show
spark.sql(s"""select sales, round((count(number_project)/$count_prjts * 100),2) as Total_Project_Per_Department_Share, count(number_project)/$count_prjts * 100 > 40 as Total_Projects_Greater_Than_40 from hrData group by sales order by Total_Project_Per_Department_Share desc""").show
+-----------+----------------------------------+------------------------------+
|      sales|Total_Project_Per_Department_Share|Total_Projects_Greater_Than_40|
+-----------+----------------------------------+------------------------------+
| management|                 4.200280018667911|                         false|
|         hr|                 4.926995133008867|                         false|
| accounting|                 5.113674244949664|                         false|
|      RandD|                 5.247016467764517|                         false|
|  marketing|                  5.72038135875725|                         false|
|product_mng|                6.0137342489499295|                         false|
|         IT|                 8.180545369691313|                         false|
|    support|                14.860990732715514|                         false|
|  technical|                 18.13454230282019|                         false|
|      sales|                27.601840122674844|                         false|
+-----------+----------------------------------+------------------------------+

